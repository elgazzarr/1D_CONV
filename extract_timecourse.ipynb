{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/local/softwares/anaconda3/envs/psycnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/data/local/softwares/anaconda3/envs/psycnet/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/data/local/softwares/anaconda3/envs/psycnet/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as osp\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from nilearn import datasets\n",
    "from nilearn.input_data import NiftiLabelsMasker, NiftiMapsMasker\n",
    "from nilearn import plotting\n",
    "from nilearn import image\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset created in /data/rmthomas/nilearn_data/craddock_2012\n",
      "\n",
      "Downloading data from ftp://www.nitrc.org/home/groups/cluster_roi/htdocs/Parcellations/craddock_2011_parcellations.tar.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 1868668 of 1868668 bytes (100.0%,    0.0s remaining) ...done. (7 seconds, 0 min)\n",
      "Extracting data from /data/rmthomas/nilearn_data/craddock_2012/45a4e574a5c116b8be81d080bdaa0814/craddock_2011_parcellations.tar.gz..... done.\n"
     ]
    }
   ],
   "source": [
    "x=datasets.fetch_atlas_craddock_2012()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = nib.load('/data/rmthomas/nilearn_data/craddock_2012/scorr05_2level_all.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgdata = img.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(imgdata[:,:,:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjectinfo(data_dir='/data_remote/ABIDE/', file='Phenotypic'):\n",
    "    \"\"\"\n",
    "    :param file: csv file containing subject information\n",
    "    :return: dictionary with relevant key/value pairs\n",
    "    \"\"\"\n",
    "\n",
    "    COI_ABIDEI = ['SITE_ID', 'SUB_ID', 'DX_GROUP', 'AGE_AT_SCAN', 'SEX']\n",
    "    COI_ABIDEII = ['SITE_ID', 'SUB_ID', 'DX_GROUP', 'AGE_AT_SCAN ', 'SEX'] # Notice space after AGE_AT_SCAN\n",
    "\n",
    "    pheno_file = file + \"_ABIDE_I.csv\"\n",
    "    pheno_ABIDEI = pd.read_csv(osp.join(data_dir, pheno_file), \n",
    "                               dtype={'SITE_ID':str, 'SUB_ID':int, 'DX_GROUP':int, 'AGE_AT_SCAN':float, 'SEX':str})[COI_ABIDEI]\n",
    "    pheno_ABIDEI['ABIDE_I_or_II'] = pd.Series(np.ones(len(pheno_ABIDEI))).astype(int)\n",
    "\n",
    "    pheno_file = file + \"_ABIDE_II.csv\"\n",
    "    pheno_ABIDEII = pd.read_csv(osp.join(data_dir, pheno_file), encoding='cp1252',\n",
    "                               dtype={'SITE_ID':str, 'SUB_ID':int, 'DX_GROUP':int, 'AGE_AT_SCAN':float, 'SEX':int})[COI_ABIDEII]\n",
    "    pheno_ABIDEII['ABIDE_I_or_II'] = pd.Series(2*np.ones(len(pheno_ABIDEII))).astype(int)\n",
    "\n",
    "    pheno_ABIDEII.rename(columns = {'AGE_AT_SCAN ':'AGE_AT_SCAN'}, inplace=True) # change column name to take out stupid space   \n",
    "    \n",
    "    pheno = pd.concat([pheno_ABIDEI, pheno_ABIDEII], ignore_index=True)\n",
    "    pheno['DX_GROUP'] = pheno['DX_GROUP']-1 # so that 0-> ASD, 1->CON\n",
    "    \n",
    "    return pheno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a csv file (Pandas Dataframe) for further machine learning\n",
    "\n",
    "def create_ml_csv(T1_file=None, \n",
    "                  nuisance_file='RS_denoise/nuisance_mat_18', \n",
    "                  input_root_dir='/data_local/deeplearning/ABIDE_LC',\n",
    "                  output_root_dir='/data_local/deeplearning/ABIDE_ML_inputs',\n",
    "                  time_course_dir = 'tc',\n",
    "                  subject_list_file='list_2169'):\n",
    "    \n",
    "    '''\n",
    "    subject_list_file: a file with a list of subject ids\n",
    "    root_dir: root directory with imaging and nuisance data\n",
    "    img_file: rsfmri file\n",
    "    T1_file: T1 scan file\n",
    "    nuisance_file: confounds (motion etc.,)\n",
    "    '''\n",
    "    \n",
    "    subject_list = pd.read_csv(osp.join(input_root_dir, subject_list_file), names=['SUB_ID'], header=None)\n",
    "    subject_info = get_subjectinfo()\n",
    "    \n",
    "    df_data_info = subject_info.merge(subject_list, on='SUB_ID')\n",
    "    df_data_info['SUB_ID'] = np.loadtxt(osp.join(input_root_dir, subject_list_file), dtype='str') # easier to get '00' prefix\n",
    "    #data_columns = ['ids', 'labels', 'tr', 'rsfmri', 't1', 'nuisance']\n",
    "    \n",
    "    rsfmri_bptf_file_list = []\n",
    "    rsfmri_nonbptf_file_list = []\n",
    "    nuisance_file_list = []\n",
    "    TR_list = []\n",
    "    nT_list = []\n",
    "    \n",
    "    #outputs\n",
    "    tc_file_list = [] # time course filenames\n",
    "    ccvec_file_list = [] # correlation matrix filenames\n",
    "    ccmat_file_list = [] # correlation matrix filenames\n",
    "    \n",
    "    # Check if output directory exists\n",
    "    if not osp.exists(output_root_dir):\n",
    "        os.makedirs(output_root_dir)\n",
    "        \n",
    "    \n",
    "    for sub_i in df_data_info['SUB_ID']:\n",
    "        rsfmri_bptf_file = osp.join(input_root_dir, 'raw', sub_i, 'RS_denoise/RS_clean_bptf_MNI.nii.gz')\n",
    "        rsfmri_nonbptf_file = osp.join(input_root_dir, 'raw', sub_i, 'RS_denoise/RS_clean_MNI.nii.gz')\n",
    "        \n",
    "        hdr = nib.load(rsfmri_bptf_file).header\n",
    "        \n",
    "        TR_list.append(hdr.get_zooms()[3])\n",
    "        nT_list.append(hdr.get_data_shape()[3])\n",
    "        rsfmri_bptf_file_list.append(rsfmri_bptf_file)\n",
    "        rsfmri_nonbptf_file_list.append(rsfmri_nonbptf_file)\n",
    "        nuisance_file_list.append(osp.join(input_root_dir, 'raw', sub_i, nuisance_file))\n",
    "        \n",
    "        #outputs ATLAS->generic name that is a placeholder\n",
    "        tc_file_list.append(osp.join(output_root_dir, sub_i, time_course_dir, 'ATLAS/BPTF/CONFOUNDS/timecourse.csv'))\n",
    "        ccvec_file_list.append(osp.join(output_root_dir, sub_i, time_course_dir, 'ATLAS/BPTF/CONFOUNDS/corr_vec.npy'))\n",
    "        ccmat_file_list.append(osp.join(output_root_dir, sub_i, time_course_dir, 'ATLAS/BPTF/CONFOUNDS/corr_mat.npy'))\n",
    "\n",
    "\n",
    "    df_data_info['RSFMRI_bptf_file'] = rsfmri_bptf_file_list\n",
    "    df_data_info['RSFMRI_nonbptf_file'] = rsfmri_nonbptf_file_list\n",
    "    df_data_info['TR'] = TR_list\n",
    "    df_data_info['nTimes'] = nT_list\n",
    "    df_data_info['nuisance_file'] = nuisance_file_list\n",
    "    \n",
    "    df_data_info['tc_file'] = tc_file_list\n",
    "    df_data_info['corrvec_file'] = ccvec_file_list\n",
    "    df_data_info['corrmat_file'] = ccmat_file_list\n",
    "    \n",
    "    return df_data_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_atlas(atlas_name, atlas_dir='/data/rmthomas/nilearn_data'): # HO_cort_maxprob_thr25-2mm\n",
    "\n",
    "    # Choose one of the atlases (add more when necessary)\n",
    "    # 1. AAL\n",
    "    # 2. HO_cort_maxprob_thr25-2mm\n",
    "\n",
    "\n",
    "    # Check if valid atlas name\n",
    "    if atlas_name not in ['AAL', 'HO_cort_maxprob_thr25-2mm', 'schaefer_100', 'schaefer_400',\n",
    "                         'JAMA_IC19', 'JAMA_IC52', 'JAMA_IC7']:\n",
    "        raise ValueError('atlas_name not found')\n",
    "\n",
    "    if atlas_name == 'AAL':\n",
    "        dataset = datasets.fetch_atlas_aal(version='SPM12')\n",
    "        atlas_filename = dataset.maps\n",
    "        labels = dataset.labels\n",
    "        \n",
    "    if atlas_name == 'HO_cort_maxprob_thr25-2mm':\n",
    "        dataset = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "        atlas_filename = dataset.maps\n",
    "        labels = dataset.labels[1:] # the first element is background\n",
    "        \n",
    "        \n",
    "    if atlas_name == 'schaefer_100':\n",
    "        atlas_filename = osp.join(atlas_dir,'schaefer/Schaefer2018_100Parcels_17Networks_order_FSLMNI152_2mm.nii')\n",
    "        labels = pd.read_csv(osp.join(atlas_dir, \n",
    "                                           'schaefer/Schaefer2018_100Parcels_17Networks_table.csv'))['label']\n",
    "    if atlas_name == 'schaefer_400':\n",
    "        atlas_filename = osp.join(atlas_dir, \n",
    "                                       'schaefer/Schaefer2018_400Parcels_17Networks_order_FSLMNI152_2mm.nii')\n",
    "        labels = pd.read_csv(osp.join(atlas_dir, \n",
    "                                           'schaefer/Schaefer2018_400Parcels_17Networks_table.csv'))['label']\n",
    "        \n",
    "    if atlas_name == 'JAMA_IC19':\n",
    "        atlas_filename = osp.join(atlas_dir, 'IC_JAMA/IC19.nii')\n",
    "        labels =[f'JAMA_IC19_{i+1}' for i in range(19)]\n",
    "    \n",
    "    if atlas_name == 'JAMA_IC52':\n",
    "        atlas_filename = osp.join(atlas_dir, 'IC_JAMA/IC52.nii')\n",
    "        labels =[f'JAMA_IC52_{i+1}' for i in range(52)]\n",
    "        \n",
    "    if atlas_name == 'JAMA_IC7':\n",
    "        atlas_filename = osp.join(atlas_dir, 'IC_JAMA/IC7.nii')\n",
    "        labels =[f'JAMA_IC7_{i+1}' for i in range(7)]\n",
    "        \n",
    "    return  atlas_filename, labels, len(labels)\n",
    "    #plotting.plot_roi(atlas_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass(atlas_filename, to_bandpass = False, tr=1.0, low_freq=0.01, high_freq=0.001):\n",
    "    if to_bandpass:\n",
    "        masker = NiftiLabelsMasker(labels_img=atlas_filename, standardize=True, t_r=tr, low_pass=low_freq, high_pass=high_freq)\n",
    "    else:\n",
    "        masker = NiftiLabelsMasker(labels_img=atlas_filename, standardize=True)\n",
    "        \n",
    "    return masker\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tc_corr(atlas_names, bptf, confounds,\n",
    "                    output_root_dir='/data_local/deeplearning/ABIDE_ML_inputs',\n",
    "                    time_course_dir = 'tc',\n",
    "                    to_bandpass=False):\n",
    "\n",
    "    \n",
    "    df_data_info = create_ml_csv()\n",
    "    \n",
    "    # Write the generic input and output csv files\n",
    "    df_data_info.to_csv(osp.join(output_root_dir, 'data_info.csv'))\n",
    "    \n",
    "    \n",
    "    connectivity_measure = ConnectivityMeasure(kind='correlation')\n",
    "\n",
    "    \n",
    "    nsubjects = len(df_data_info)\n",
    "    \n",
    "    print_counter = 0\n",
    "    for sub_i in df_data_info.index:\n",
    "\n",
    "        \n",
    "        if print_counter%100 == 0:\n",
    "            print(f'{sub_i}/{nsubjects}')\n",
    "        nuisance = pd.read_csv(df_data_info['nuisance_file'].loc[sub_i], sep='\\t', header=None)\n",
    "        nuisance.to_csv('temp_nuisance.csv') # required for the next step in csv format\n",
    "\n",
    "        for atlas_name in atlas_names:\n",
    "            \n",
    "            atlas_filename, labels, nrois = select_atlas(atlas_name)\n",
    "            \n",
    "            if atlas_name in ['JAMA_IC19', 'JAMA_IC52', 'JAMA_IC7']:\n",
    "                masker = NiftiMapsMasker(atlas_filename,  standardize=True, memory='nilearn_cache')\n",
    "                \n",
    "            else:\n",
    "                if to_bandpass:\n",
    "                        masker = bandpass(atlas_filename, to_bandpass=to_bandpass, tr=df_data_info['TR'].loc[sub_i])\n",
    "                else:\n",
    "                        masker = bandpass(atlas_filename, to_bandpass=to_bandpass)\n",
    "\n",
    "            if bptf:\n",
    "                if confounds:\n",
    "                    atlas_bptf_conf_name = osp.join(atlas_name, 'bptf/nilearn_regress')\n",
    "                    timecourse = masker.fit_transform(imgs=df_data_info['RSFMRI_bptf_file'].loc[sub_i], \n",
    "                                              confounds='temp_nuisance.csv')\n",
    "                    \n",
    "                else:\n",
    "                    atlas_bptf_conf_name = osp.join(atlas_name, 'bptf/no_nilearn_regress')\n",
    "                    timecourse = masker.fit_transform(imgs=df_data_info['RSFMRI_bptf_file'].loc[sub_i]) \n",
    "             \n",
    "            else:\n",
    "                if confounds:\n",
    "                    atlas_bptf_conf_name = osp.join(atlas_name, 'no_bptf/nilearn_regress')\n",
    "                    timecourse = masker.fit_transform(imgs=df_data_info['RSFMRI_nonbptf_file'].loc[sub_i], \n",
    "                                              confounds='temp_nuisance.csv')\n",
    "                else:\n",
    "                    atlas_bptf_conf_name = osp.join(atlas_name, 'no_bptf/no_nilearn_regress')\n",
    "                    timecourse = masker.fit_transform(imgs=df_data_info['RSFMRI_nonbptf_file'].loc[sub_i]) \n",
    "            \n",
    "\n",
    "            cc = connectivity_measure.fit_transform([timecourse])[0]\n",
    "\n",
    "            tc_dir = osp.join(output_root_dir, df_data_info['SUB_ID'].loc[sub_i], \n",
    "                              time_course_dir, atlas_bptf_conf_name)\n",
    "            \n",
    "            if not osp.exists(tc_dir):\n",
    "                os.makedirs(tc_dir)\n",
    "\n",
    "            # Write timeseries as csv file\n",
    "            tc_file = df_data_info['tc_file'].loc[sub_i].replace('ATLAS/BPTF/CONFOUNDS', atlas_bptf_conf_name)\n",
    "            pd.DataFrame(data=timecourse, columns=labels).to_csv(tc_file, index=False)\n",
    "\n",
    "            # Write correlation matrix\n",
    "            corrvec_file = df_data_info['corrvec_file'].loc[sub_i].replace('ATLAS/BPTF/CONFOUNDS', atlas_bptf_conf_name)\n",
    "            corrmat_file = df_data_info['corrmat_file'].loc[sub_i].replace('ATLAS/BPTF/CONFOUNDS', atlas_bptf_conf_name)\n",
    "            cc_triu_ids = np.triu_indices(nrois)\n",
    "            \n",
    "            np.save(corrvec_file, cc[cc_triu_ids]) # get only upper triangular\n",
    "            np.save(corrmat_file, cc)\n",
    "            \n",
    "            \n",
    "            \n",
    "        print_counter += 1\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    \n",
    "    atlas_names=['JAMA_IC19', 'JAMA_IC52', 'JAMA_IC7', 'AAL', 'HO_cort_maxprob_thr25-2mm', 'schaefer_100', 'schaefer_400']\n",
    "\n",
    "    output_root_dir='/data_local/deeplearning/ABIDE_ML_inputs'\n",
    "    time_course_dir = 'tc'\n",
    "    \n",
    "    for confs in [True, False]:\n",
    "        extract_tc_corr(atlas_names, bptf=False, confounds=confs, \n",
    "                        output_root_dir=output_root_dir, time_course_dir=time_course_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2169\n",
      "100/2169\n",
      "200/2169\n",
      "300/2169\n",
      "400/2169\n",
      "500/2169\n",
      "600/2169\n",
      "700/2169\n",
      "800/2169\n",
      "900/2169\n",
      "1000/2169\n",
      "1100/2169\n",
      "1200/2169\n",
      "1300/2169\n",
      "1400/2169\n",
      "1500/2169\n",
      "1600/2169\n",
      "1700/2169\n",
      "1800/2169\n",
      "1900/2169\n",
      "2000/2169\n",
      "2100/2169\n",
      "0/2169\n",
      "100/2169\n",
      "200/2169\n",
      "300/2169\n",
      "400/2169\n",
      "500/2169\n",
      "600/2169\n",
      "700/2169\n",
      "800/2169\n",
      "900/2169\n",
      "1000/2169\n",
      "1100/2169\n",
      "1200/2169\n",
      "1300/2169\n",
      "1400/2169\n",
      "1500/2169\n",
      "1600/2169\n",
      "1700/2169\n",
      "1800/2169\n",
      "1900/2169\n",
      "2000/2169\n",
      "2100/2169\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "\n",
    "df = create_ml_csv()\n",
    "# plot ntimes to check what to include\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,10))\n",
    "df.hist(column='nTimes', bins=[50, 100, 125, 150, 200, 300, 500, 1000], ax=axes[0])\n",
    "df.hist(column='nTimes', cumulative=-1, bins=[50, 100, 125, 150, 200, 300, 1000], linewidth=5, histtype='step', ax=axes[1])\n",
    "#df_input_data['nTimes'].plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=datasets.fetch_atlas_aal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=datasets.fetch_atlas_harvard_oxford(atlas_name='cort-maxprob-thr25-2mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xx.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
