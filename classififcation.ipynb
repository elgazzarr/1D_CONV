{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification of ASD vs Controls based on different atlases.\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pprint import pprint\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os.path as osp\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Abide1DConvNet(nn.Module):\n",
    "    def __init__(self, nROIS=2):\n",
    "        super(Abide1DConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(nROIS, 64, 7)\n",
    "        self.avg = nn.AdaptiveAvgPool1d((1))\n",
    "        #self.drop2 = nn.Dropout(p=0.0)\n",
    "        self.linear1 = nn.Linear(64, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.avg(x).view(-1, 64)\n",
    "        #x = self.drop2(x)\n",
    "        x =self.linear1(x)\n",
    "        x = F.softmax(x,dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(net, val_data_loader, criterion=nn.CrossEntropyLoss().cuda()):\n",
    "     \n",
    "    net.eval()\n",
    "    loss = 0.0\n",
    "    labels = np.empty([],dtype=int)\n",
    "    predictions = np.empty([],dtype=int)\n",
    "    for i, (tc, dx) in enumerate(val_data_loader):\n",
    "\n",
    "            tc = Variable(tc).type(torch.cuda.FloatTensor)\n",
    "            dx = Variable(dx).type(torch.cuda.LongTensor)\n",
    "   \n",
    "            # forward pass\n",
    "            output = net(tc)\n",
    "\n",
    "            # calculate loss\n",
    "            loss += criterion(output, torch.max(dx,1)[1])\n",
    "            \n",
    "            #append labels and predictions of each batch\n",
    "            labels = np.append(labels,torch.argmax(dx,1).cpu().numpy())\n",
    "            predictions = np.append(predictions,torch.argmax(output,1).cpu().numpy())\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm1 = confusion_matrix(labels[1:] , predictions[1:])\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "\n",
    "\n",
    "    return loss/len(val_data_loader), accuracy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data,val_data,exp_dir,nepochs,verbose =True):\n",
    "    \n",
    "    \n",
    "    train_data_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_data_loader = DataLoader(val_data, batch_size=32, shuffle=True)\n",
    "\n",
    "    \n",
    "    nrois = train_data.__getitem__(0)[0].shape[0] # Trick to get the nrois (=nchannels)\n",
    "    \n",
    "    # Calculating class distribution\n",
    "    \n",
    "    class_db = (np.unique(np.argmax(train_data.tensors[1].numpy(),1), return_counts=True))[1]\n",
    "    class_db = class_db[::-1]\n",
    "    class_weigths = torch.tensor((class_db/np.sum(class_db)),dtype=torch.float).cuda()\n",
    "\n",
    "\n",
    "    \n",
    "    net = Abide1DConvNet(nROIS=nrois).cuda()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weigths)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=.0005,weight_decay=0.02)\n",
    "    \n",
    "    net.train()\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    best_val_loss = None\n",
    "    best_net = None\n",
    "    \n",
    "    #print(f'Training ...')\n",
    "    for i_epoch in range(nepochs):\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        for i, (tc, dx) in enumerate(train_data_loader):\n",
    "\n",
    "            tc = Variable(tc).type(torch.cuda.FloatTensor)\n",
    "            dx = Variable(dx).type(torch.cuda.LongTensor)\n",
    "            # forward pass\n",
    "            output = net(tc).cuda(1)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = criterion(output.cuda(1), torch.max(dx,1)[1].cuda(1)).cuda(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "\n",
    "        epoch_train_loss = epoch_loss/i\n",
    "        epoch_val_loss, epoch_val_acc = validate_model(net, val_data_loader,criterion=criterion)\n",
    "        \n",
    "        train_loss.append(epoch_train_loss)\n",
    "        val_loss.append(epoch_val_loss)\n",
    "        \n",
    "        #if verbose and i_epoch%1 == 0:\n",
    "            #print('Epoch:{} --- Train_loss:{} --- Val_loss:{} --- Val_acc:{}'.format(i_epoch, epoch_train_loss, epoch_val_loss, epoch_val_acc))\n",
    "        \n",
    "        #Save model with best validation loss\n",
    "        if not best_val_loss or epoch_val_loss < best_val_loss:\n",
    "            with open(os.path.join(exp_dir,\"model.pt\"), 'wb') as f:\n",
    "                #print(\"saving best model....\")\n",
    "                torch.save(net, f)\n",
    "            best_net = net\n",
    "            best_val_loss = epoch_val_loss\n",
    "                                    \n",
    "    return best_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net,test_data):\n",
    "    test_data_loader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "    net.cuda().eval()\n",
    "    tot_acc = 0.0\n",
    "    tot_spec = 0.0\n",
    "    labels = np.empty([],dtype=int)\n",
    "    predictions = np.empty([],dtype=int)\n",
    "    for i, (tc, dx) in enumerate(test_data_loader):\n",
    "\n",
    "            tc = Variable(tc).type(torch.cuda.FloatTensor)\n",
    "            dx = Variable(dx).type(torch.cuda.LongTensor)\n",
    "\n",
    "            # forward pass\n",
    "            output = net(tc).cuda()\n",
    "            labels = np.append(labels,torch.argmax(dx,1).cpu().numpy())\n",
    "            predictions = np.append(predictions,torch.argmax(output,1).cpu().numpy())\n",
    "            \n",
    "\n",
    "    cm1 = confusion_matrix(labels[1:] , predictions[1:])\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "\n",
    "    return accuracy1,sensitivity1,specificity1,labels[1:],predictions[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for training the model and validating it based on k-fold validation scheme\n",
    "def run_kfold(\n",
    "    exp_dir='/data/agelgazzar/projects/models/cv/exp1/',\n",
    " atlas_name='schaefer_400',\n",
    " root_dir='/data_local/deeplearning/ABIDE_ML_inputs/', \n",
    " data_info_file='data_info_correct.csv', \n",
    " dir1= \"bptf\",\n",
    " dir2 = \"no_nilearn_regress\",\n",
    " nTime_min=200, \n",
    " zscore=True,\n",
    " folds = 10,\n",
    " epochs = 300):    \n",
    "\n",
    "        \n",
    "        \n",
    "        # Check if valid atlas name\n",
    "        if atlas_name not in ['AAL', 'HO','HO_cort_maxprob_thr25-2mm', 'schaefer_100', 'schaefer_400','JAMA_IC19','JAMA_IC52',\"JAMA_IC7\"]:\n",
    "            raise ValueError('atlas_name not found')\n",
    "        \n",
    "        #print(\"preparing dataset ....\")\n",
    "        \n",
    "        # Read the parent CSV file\n",
    "        \n",
    "        data_info = pd.read_csv(osp.join(root_dir, data_info_file))\n",
    "        \n",
    "        #nTime_max = 250\n",
    "        #data_info_new = data_info[data_info.nTimes > nTime_max]\n",
    "        #max_subjects = len(data_info_new)\n",
    "        \n",
    "        \n",
    "        ## Filter out badly preprocessed samples\n",
    "        text_file = open(\"/data_local/deeplearning/ABIDE_LC/dualreg_classification/list_nogood.txt\", \"r\")\n",
    "        lines = text_file.read().split('\\n')\n",
    "        ind = [int(i) for i in lines[:-1]]\n",
    "        data_info = data_info[~ np.isin(data_info[\"SUB_ID\"],ind)]\n",
    "        \n",
    "\n",
    "        # Filter the dataframe to contain subjects with nTimes > nTime threhsold\n",
    "\n",
    "        data_info = data_info[data_info.nTimes > nTime_min]\n",
    "\n",
    "\n",
    "        #data_info = data_info.sample(max_subjects)\n",
    "\n",
    "        data_info = shuffle(data_info,random_state = 1)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        # Determine the nchannels (=nrois) from the data by using the first sample\n",
    "        sample_file = data_info['tc_file'].iloc[0].replace('ATLAS', atlas_name).replace(\"BPTF\",dir1).replace(\"CONFOUNDS\",dir2)\n",
    "        nrois = pd.read_csv(sample_file).values.shape[1]\n",
    "        \n",
    "        total_subjects = len(data_info)\n",
    "        \n",
    "        \n",
    "        # Initialize an np array to store all timecourses and labels\n",
    "        tc_data = np.zeros((total_subjects, nrois, nTime_min))\n",
    "        labels = np.zeros(total_subjects, dtype=int)\n",
    "        ids = np.zeros(total_subjects, dtype=int)\n",
    "\n",
    "        \n",
    "        # Load data       \n",
    "        for i, sub_i in enumerate(data_info.index):\n",
    "            tc_file = data_info['tc_file'].loc[sub_i].replace('ATLAS', atlas_name).replace(\"BPTF\",dir1).replace(\"CONFOUNDS\",dir2)\n",
    "            tc_vals = pd.read_csv(tc_file).values.transpose()[:, :nTime_min]\n",
    "\n",
    "            if (zscore):       \n",
    "                tc_vals =  np.array([(tc_vals[:,i] - np.mean(tc_vals[:,i]))/np.std(tc_vals[:,i]) for i in range (tc_vals.shape[1])])\n",
    "                tc_data[i] = tc_vals.transpose()\n",
    "            else:\n",
    "                tc_data[i] = tc_vals\n",
    "\n",
    "            labels[i] = data_info['DX_GROUP'].loc[sub_i]\n",
    "            ids[i] = data_info['SUB_ID'].loc[sub_i]\n",
    "            \n",
    "            \n",
    "            \n",
    "   \n",
    "        labels = np.eye(2)[labels]\n",
    "        \n",
    "        kfold = KFold(folds, True, 1)\n",
    "        \n",
    "        j = 0\n",
    "        \n",
    "        total_accuracy = 0\n",
    "        total_sensitivity = 0\n",
    "        total_specificity = 0\n",
    "        \n",
    "        accuracies = np.zeros(10,dtype=float)\n",
    "        lbls_csv = np.empty([],dtype=int)\n",
    "        preds_csv = np.empty([],dtype=int)\n",
    "        ids_csv = np.empty([],dtype=str)\n",
    "        #K-fold Cross_validation\n",
    "        for train_index, test_index in kfold.split(tc_data):\n",
    "            path = osp.join(exp_dir,\"{}/{}/fold{}\".format(str(nTime_min),atlas_name,str(j)))\n",
    "            if not osp.exists(path):\n",
    "                os.makedirs(path)\n",
    "            #spltitting training fold into 90% training and 10% validation    \n",
    "            train_split = int(0.9 * len(train_index))\n",
    "            train_i = train_index[0:train_split]\n",
    "            val_i = train_index[train_split:]\n",
    "            \n",
    "            # Create training,testing and validation datasets\n",
    "            train_data = torch.from_numpy(tc_data[train_i])\n",
    "            train_labels= torch.from_numpy(labels[train_i])\n",
    "            val_data = torch.from_numpy(tc_data[val_i])\n",
    "            val_labels = torch.from_numpy(labels[val_i])\n",
    "            test_data = torch.from_numpy(tc_data[test_index])\n",
    "            test_labels = torch.from_numpy(labels[test_index])\n",
    "            test_ids = ids[test_index]\n",
    "            train = data_utils.TensorDataset(train_data, train_labels)\n",
    "            val = data_utils.TensorDataset(val_data, val_labels)\n",
    "            test = data_utils.TensorDataset(test_data, test_labels)\n",
    "            \n",
    "            #train network\n",
    "            trained_network = train_model(train,val,path,epochs)\n",
    "            #test network\n",
    "            test_accuracy, test_sens, test_spec,fold_labels,fold_preditions = test_model(trained_network,test)\n",
    "            lbls_csv = np.append(lbls_csv,fold_labels)\n",
    "            preds_csv = np.append(preds_csv,fold_preditions)\n",
    "            ids_csv = np.append(ids_csv,test_ids)\n",
    "            total_accuracy += test_accuracy\n",
    "            total_sensitivity += test_sens\n",
    "            total_specificity += test_spec\n",
    "            accuracies[j] = round(test_accuracy,3)\n",
    "            j +=1\n",
    "            #print(\"----Test results of of fold {} are : {} acc., {} sens. and  {} spec. ----\".format(j, test_accuracy, test_sens, test_spec))\n",
    "            \n",
    "            \n",
    "        acc = total_accuracy/folds\n",
    "        sens = total_sensitivity/folds\n",
    "        spec = total_specificity/folds\n",
    "        print(\"{} in {} nTime results are: {} acc., {} sens. and  {} spec. \".format(atlas_name, nTime_min, round(acc,2), round(sens,2), round(spec,2)))\n",
    "        print(len(ids_csv))\n",
    "        print(len(lbls_csv))\n",
    "        print(len(preds_csv))\n",
    "        dic = {'SUB_ID':ids_csv[1:],'Label':lbls_csv[1:],'Pred':preds_csv[1:]}\n",
    "        df = pd.DataFrame(dic)\n",
    "        df.to_csv('Results.csv')\n",
    "        return accuracies,total_subjects\n",
    "            \n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used for training the model and validating it based on leave one site out validation scheme\n",
    "\n",
    "def run_site_val(\n",
    "    exp_dir='/data/agelgazzar/projects/models/cv/exp1/',\n",
    " atlas_name='schaefer_400',\n",
    " root_dir='/data_local/deeplearning/ABIDE_ML_inputs/', \n",
    " data_info_file='data_info_correct.csv', \n",
    " dir1= \"bptf\",\n",
    " dir2 = \"no_nilearn_regress\",\n",
    " nTime_min=200, \n",
    " zscore=True,\n",
    " folds = 10,\n",
    " epochs = 100):    \n",
    "\n",
    "        \n",
    "        \n",
    "        # Check if valid atlas name\n",
    "        if atlas_name not in ['AAL','HO', 'HO_cort_maxprob_thr25-2mm', 'schaefer_100', 'schaefer_400','JAMA_IC19','JAMA_IC52',\"JAMA_IC7\"]:\n",
    "            raise ValueError('atlas_name not found')\n",
    "        \n",
    "        #print(\"preparing dataset ....\")\n",
    "        # Read the parent CSV file\n",
    "        data_info = pd.read_csv(osp.join(root_dir, data_info_file))\n",
    "        \n",
    "        #nTime_max = 250   \n",
    "        #data_info_new = data_info[data_info.nTimes > nTime_max]\n",
    "        #max_subjects = len(data_info_new)\n",
    "        \n",
    "        # Filter the dataframe to contain subjects with nTimes > ntime threshold\n",
    "        data_info = data_info[data_info.nTimes > nTime_min]\n",
    "        \n",
    "        data_info = shuffle(data_info,random_state = 1)\n",
    "\n",
    "        # filter out sites with number of subjects less than 10 \n",
    "        sites,counts = np.unique(data_info[\"SITE_ID\"].values,  return_counts=True)\n",
    "        filtered_sites = sites[np.where(counts>5)[0]]\n",
    "        data_info_filtered = data_info[np.isin(data_info[\"SITE_ID\"],filtered_sites)]\n",
    "          \n",
    "        \n",
    "        # Determine the nchannels (=nrois) from the data by using the first sample\n",
    "        sample_file = data_info_filtered['tc_file'].iloc[0].replace('ATLAS', atlas_name).replace(\"BPTF\",dir1).replace(\"CONFOUNDS\",dir2)\n",
    "        nrois = pd.read_csv(sample_file).values.shape[1]\n",
    "        \n",
    "        \n",
    "        # Initialize an np array to store all timecourses and labels\n",
    "        total_subjects = len(data_info_filtered)    \n",
    "        tc_data = np.zeros((total_subjects, nrois, nTime_min))\n",
    "        labels = np.zeros(total_subjects, dtype=int)\n",
    "\n",
    "        \n",
    "        # Load data\n",
    "        for i, sub_i in enumerate(data_info_filtered.index):\n",
    "            tc_file = data_info_filtered['tc_file'].loc[sub_i].replace('ATLAS', atlas_name).replace(\"BPTF\",dir1).replace(\"CONFOUNDS\",dir2)\n",
    "            tc_vals = pd.read_csv(tc_file).values.transpose()[:, :nTime_min]\n",
    "\n",
    "            if (zscore):       \n",
    "                tc_vals =  np.array([(tc_vals[:,i] - np.mean(tc_vals[:,i]))/np.std(tc_vals[:,i]) for i in range (tc_vals.shape[1])])\n",
    "                tc_data[i] = tc_vals.transpose()\n",
    "            else:\n",
    "                tc_data[i] = tc_vals     \n",
    "                \n",
    "            labels[i] = data_info_filtered['DX_GROUP'].loc[sub_i]\n",
    "       \n",
    "        # One-hot enconding of labels\n",
    "        labels = np.eye(2)[labels]\n",
    "        \n",
    "        \n",
    "        accuracies = np.zeros(len(filtered_sites),dtype=float)\n",
    "        j = 0\n",
    "        #Site Cross_validation\n",
    "        for site in filtered_sites:\n",
    "            \n",
    "            test_index = np.where(data_info_filtered[\"SITE_ID\"] == site)[0]\n",
    "            size = len(test_index)\n",
    "            train_index = np.where(data_info_filtered[\"SITE_ID\"] != site)[0]\n",
    "            np.random.shuffle(train_index)\n",
    "            \n",
    "            path = osp.join(exp_dir,\"{}/{}/{}\".format(str(nTime_min),atlas_name,site))\n",
    "            if not osp.exists(path):\n",
    "                os.makedirs(path)\n",
    "            \n",
    "            #spltitting training fold into 90% training and 10% validation    \n",
    "            train_split = int(0.9 * len(train_index))\n",
    "            train_i = train_index[0:train_split]\n",
    "            val_i = train_index[train_split:]\n",
    "            tl = np.argmax(labels[test_index],axis=1)\n",
    "            size = len(tl)\n",
    "            control = np.count_nonzero(tl == 0)\n",
    "            asd = np.count_nonzero(tl == 1)\n",
    "            \n",
    "            \n",
    "            # Create training,testing and validation datasets\n",
    "            train_data = torch.from_numpy(tc_data[train_i])\n",
    "            train_labels  = torch.from_numpy(labels[train_i])\n",
    "            val_data = torch.from_numpy(tc_data[val_i])\n",
    "            val_labels = torch.from_numpy(labels[val_i])\n",
    "            test_data = torch.from_numpy(tc_data[test_index])\n",
    "            test_labels = torch.from_numpy(labels[test_index])\n",
    "            train = data_utils.TensorDataset(train_data, train_labels)\n",
    "            val  = data_utils.TensorDataset(val_data, val_labels)\n",
    "            test = data_utils.TensorDataset(test_data, test_labels)\n",
    "            # train network\n",
    "            trained_network = train_model(train,val,path,epochs)\n",
    "            # test network \n",
    "            test_accuracy, test_sens, test_spec = test_model(trained_network,test)\n",
    "            accuracies[j] = round(test_accuracy,3)\n",
    "            j+=1\n",
    "            #print(\"{} :acc {}, sens {},  spec {}\".format(site, test_accuracy, test_sens, test_spec))\n",
    "            print('{} ... acc:{}, sens:{},  spec:{}..... size:{}, ASD:{} ,TD:{}'.format(site,round(test_accuracy,2),round(test_sens,2), round(test_spec,2), size,asd,control))\n",
    "        \n",
    "        return filtered_sites,accuracies\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/local/softwares/anaconda3/envs/leotorch/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Abide1DConvNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAL in 100 nTime results are: 0.6 acc., 0.61 sens. and  0.59 spec. \n",
      "AAL in 150 nTime results are: 0.6 acc., 0.62 sens. and  0.59 spec. \n",
      "AAL in 200 nTime results are: 0.63 acc., 0.55 sens. and  0.71 spec. \n",
      "AAL in 250 nTime results are: 0.6 acc., 0.57 sens. and  0.63 spec. \n",
      "AAL in 300 nTime results are: 0.58 acc., 0.42 sens. and  0.68 spec. \n",
      "-------------------------------------------\n",
      "HO_cort_maxprob_thr25-2mm in 100 nTime results are: 0.58 acc., 0.62 sens. and  0.55 spec. \n",
      "HO_cort_maxprob_thr25-2mm in 150 nTime results are: 0.61 acc., 0.58 sens. and  0.64 spec. \n",
      "HO_cort_maxprob_thr25-2mm in 200 nTime results are: 0.67 acc., 0.65 sens. and  0.69 spec. \n",
      "HO_cort_maxprob_thr25-2mm in 250 nTime results are: 0.56 acc., 0.59 sens. and  0.56 spec. \n",
      "HO_cort_maxprob_thr25-2mm in 300 nTime results are: 0.68 acc., 0.6 sens. and  0.75 spec. \n",
      "-------------------------------------------\n",
      "schaefer_100 in 100 nTime results are: 0.59 acc., 0.51 sens. and  0.67 spec. \n",
      "schaefer_100 in 150 nTime results are: 0.6 acc., 0.54 sens. and  0.65 spec. \n",
      "schaefer_100 in 200 nTime results are: 0.61 acc., 0.66 sens. and  0.59 spec. \n",
      "schaefer_100 in 250 nTime results are: 0.62 acc., 0.56 sens. and  0.67 spec. \n",
      "schaefer_100 in 300 nTime results are: 0.66 acc., 0.49 sens. and  0.77 spec. \n",
      "-------------------------------------------\n",
      "schaefer_400 in 100 nTime results are: 0.61 acc., 0.57 sens. and  0.66 spec. \n",
      "schaefer_400 in 150 nTime results are: 0.63 acc., 0.59 sens. and  0.66 spec. \n",
      "schaefer_400 in 200 nTime results are: 0.64 acc., 0.62 sens. and  0.67 spec. \n",
      "schaefer_400 in 250 nTime results are: 0.61 acc., 0.56 sens. and  0.67 spec. \n",
      "schaefer_400 in 300 nTime results are: 0.58 acc., 0.42 sens. and  0.71 spec. \n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ntimes  = [100,150,200,250,300]\n",
    "atlases = ['AAL', 'HO_cort_maxprob_thr25-2mm', 'schaefer_100', 'schaefer_400']\n",
    "col_names =  ['atlas','nTime','accuracies',  'N']\n",
    "df  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "\n",
    "for atlas in atlases:\n",
    "    for ntime in ntimes: \n",
    "        accs, n_subjects = run_kfold(atlas_name=atlas,nTime_min=ntime)\n",
    "        df.loc[len(df)] = {'atlas':atlas,'nTime':ntime,'accuracies':accs, 'N':n_subjects}\n",
    "        df.to_csv('ABIDE_evaluation.csv')\n",
    "          \n",
    "    print('-------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
